# Проект: Расширение аналитического DWH и расчет маркетинговых метрик вовлеченности

## Описание проекта
Цель проекта — расширение существующего аналитического хранилища на базе **Vertica** новыми источниками данных и подготовка отчетности для отдела маркетинга. В рамках работы реализован процесс интеграции логов активности групп из облачного S3-хранилища (`group_log.csv`) и расчет конверсии участников сообществ в активных пользователей.

**Ключевые этапы реализации:**
*   **Автоматизация ETL**: Разработка Airflow DAG для выгрузки данных из облачного S3-хранилища и их загрузки в Vertica.
*   **Data Modeling (Data Vault)**: Проектирование и наполнение новых сущностей (Link и Satellite) для хранения истории активности пользователей.
*   **Маркетинговая аналитика**: Расчет воронки конверсии («вступление ➔ первое сообщение») для сегмента старейших сообществ соцсети.


## Архитектура и поток данных

В проекте реализован классический путь данных в аналитическом контуре:

```text
 [ S3 Bucket ] ──► [ Airflow (DAG) ] ──► [ Vertica STG ] ──► [ Vertica DWH (Vault) ] ──► [ Marketing Mart ]
 (Логи событий)    (Загрузка данных)     (Сырой слой)        (Hubs, Links, Sats)       (Расчет CR)
```
*   **Слой STAGING**: Приземление сырых данных из CSV-файлов во временные таблицы аналитической СУБД.
*   **Слой DDS (Data Vault)**:
    *   Наполнение линка `l_user_group_activity` для фиксации связей пользователей и групп.
    *   Создание сателлита `s_auth_history` для хранения атрибутов и точного времени событий.
*   **Слой CDM (Analytics)**: Использование временных таблиц (CTE) для агрегации уникальных участников и их активности с последующим расчетом доли вовлеченных пользователей.

## Навыки и инструменты

### Python и Airflow (Оркестрация)
*   **Cloud Integration**: Работа с Yandex Cloud S3, автоматизация загрузки объектов через `boto3`.
*   **ETL Engineering**: Написание Python-операторов для оркестрации процессов «выгрузка — локальное хранение — загрузка в БД».
*   **Vertica Interaction**: Использование библиотеки `vertica_python` для прямого управления процессами загрузки данных и выполнения DDL/DML запросов.

### SQL и Аналитические БД (Vertica)
*   **Data Modeling (Data Vault)**: Проектирование Хабов, Линков и Сателлитов. Работа с хэшированными ключами (`hk`) для оптимизации Join-операций.
*   **Physical Optimization**: Управление распределением данных через **SEGMENTED BY HASH** (распределение по нодам кластера) и **PARTITION BY** (иерархическое хранение данных по времени).
*   **High-Speed Loading**: Реализация пакетной загрузки через `COPY FROM LOCAL`, обеспечивающей максимальную пропускную способность при импорте CSV.
*   **Advanced Analytics**: Написание структурированных SQL-запросов (CTE, вложенные запросы, агрегаты) для получения бизнес-ответов из сырых логов.


## Общий вывод
В ходе проекта успешно внедрена новая модель хранения исторических данных, позволившая детально проанализировать поведение пользователей. Использование архитектуры Data Vault обеспечило гибкость системы, а инструменты оптимизации Vertica (партиционирование и сегментация) — высокую скорость выполнения аналитических запросов.
Главным итогом стал расчет воронки конверсии, выявивший наиболее эффективные сообщества для вовлечения аудитории. 

