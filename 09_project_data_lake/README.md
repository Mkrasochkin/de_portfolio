# Базовая геоаналитика в Data Lake (PySpark & Airflow)

## Описание проекта
В рамках проекта реализован процесс обработки логов событий социальной сети, хранящихся в **Hadoop (HDFS)**. Основная задача заключалась в том, чтобы научиться работать с географическими координатами пользователей и рассчитывать простые метрики вовлеченности на больших объемах данных с помощью **Apache Spark**.

**Что было сделано в ходе проекта:**
*   **Обработка координат**: Расчет расстояний от сообщений пользователей до ближайших городов Австралии.
*   **Определение локаций**: Попытка выделить «домашний город» пользователя на основе длительности его пребывания в одной точке (более 27 дней).
*   **Система рекомендаций**: Реализация простого алгоритма поиска потенциальных друзей, которые находятся в радиусе 1 км и имеют общие интересы (подписки на каналы).
*   **Автоматизация**: Сборка написанных Spark-скриптов в единую цепочку задач (DAG) в Airflow.


## Архитектура и поток данных

Проект построен по принципу постепенной трансформации данных внутри Data Lake:

```text
 [ HDFS Raw ] ──► [ PySpark Скрипты ] ──► [ HDFS Analytics ] ──► [ Airflow DAG ]
 (Сырые логи)      (Расчеты и Join-ы)     (Итоговые витрины)     (Запуск по расписанию)
```

1. **Загрузка**: Данные о событиях и справочник городов Австралии вычитываются из HDFS.
2. **Гео-фильтрация**: Для каждого сообщения вычисляется расстояние до центров городов (использовалась формула гаверсинуса).
3. **Агрегация**: Данные группируются по неделям и месяцам для подсчета количества сообщений, реакций и регистраций в каждом городе.
4. **Оркестрация**: Запуск расчетов автоматизирован через SparkSubmitOperator в Airflow.



## Навыки и инструменты

### Работа с Big Data (PySpark & HDFS)
* **Основы PySpark**: Практика использования Spark-трансформаций для фильтрации и объединения таблиц. Работа с оконными функциями (Window) для анализа последовательности событий.
* **Гео-данные**: Применение математических функций в Spark для вычисления расстояний между координатами.
* **HDFS**: Загрузка файлов и сохранение результатов в оптимизированном формате Parquet с использованием партиционирования.

### Оркестрация (Airflow)
* **Автоматизация**: Настройка DAG для последовательного запуска Spark-приложений.
* **Управление ресурсами**: Знакомство с параметрами запуска задач на кластере (настройка памяти и ядер для экзекуторов).


## Общий вывод
В ходе проекта удалось на практике познакомиться с инструментами Big Data. Были изучены базовые принципы работы Apache Spark: как читать данные из распределенной системы, как связывать большие таблицы между собой и как автоматизировать эти процессы.
Самым сложным этапом стала реализация логики «домашнего города» и расчет расстояний, что потребовало разбора работы оконных функций и математических формул внутри Spark. Проект помог понять общую структуру Data Lake и то, как распределенные вычисления ускоряют обработку миллионов строк данных.


